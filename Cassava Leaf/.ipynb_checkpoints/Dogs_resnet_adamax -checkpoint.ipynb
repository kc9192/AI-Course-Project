{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stanford_dogs with resnet and adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Loading and summarizing data and creating data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dataset_info = tfds.load('cassava', split='train+test+validation', as_supervised = True, with_info = True)\n",
    "datalen = [i for i,_ in enumerate(data)][-1] + 1\n",
    "datalen\n",
    "\n",
    "train_split = int(0.6*datalen)\n",
    "test_val_split = int(0.2*datalen)\n",
    "test_val_split = int(0.2*datalen)\n",
    "\n",
    "training_set = data.take(train_split)\n",
    "test_set = data.skip(train_split)\n",
    "validation_set = test_set.skip(test_val_split)\n",
    "test_set = test_set.take(test_val_split)\n",
    "\n",
    "dataset=training_set,validation_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above datset_info we can see that the data is split into train, test and validation batches and each image has features like filename, label and image itself(255 pixels) which can be harnessed whenever needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples = dataset_info.splits['train'].num_examples + dataset_info.splits['test'].num_examples + dataset_info.splits['validation'].num_examples\n",
    "\n",
    "num_training_examples = train_split\n",
    "num_validation_examples = test_val_split\n",
    "num_test_examples = num_validation_examples\n",
    "\n",
    "print('There are {:,} images in the training set'.format(num_training_examples))\n",
    "print('There are {:,} images in the validation set'.format(num_validation_examples))\n",
    "print('There are {:,} images in the test set'.format(num_test_examples))\n",
    "\n",
    "num_classes = dataset_info.features['label'].num_classes\n",
    "print('There are {:,} classes in our dataset'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [224,224], preserve_aspect_ratio=False)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "batch_size = 32\n",
    "image_size = 224\n",
    "\n",
    "training_batches = training_set.cache().shuffle(num_training_examples//4).map(resize).batch(batch_size).prefetch(1)\n",
    "validation_batches = validation_set.cache().map(resize).batch(batch_size).prefetch(1)\n",
    "testing_batches = test_set.cache().map(resize).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Creating models, plots and test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "decay_steps = 100000\n",
    "decay_rate = 0.95\n",
    "\n",
    "exp_lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(lr, decay_steps, decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(history):\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    " \n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range=range(EPOCHS)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, training_accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, training_loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_plot(pretrained_model,losses,accuracies):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(all_optimizers, losses)\n",
    "    plt.xlabel(\"Gradient Descent Variants\")\n",
    "    plt.ylabel(\"Loss Values\")\n",
    "    plt.title('Testing Loss Summary')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(all_optimizers, accuracies)\n",
    "    plt.xlabel(\"Gradient Descent Variants\")\n",
    "    plt.ylabel(\"Accuracy Values\")\n",
    "    plt.title('Testing Accuracy Summary')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "feature_extractor = hub.KerasLayer(pretrained_model, input_shape=(image_size, image_size,3))\n",
    "feature_extractor.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "        feature_extractor,\n",
    "        tf.keras.layers.Dense(num_classes, activation ='softmax')])\n",
    "    \n",
    "           \n",
    "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=exp_lr_scheduler),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "history = model.fit(training_batches,\n",
    "                    epochs = EPOCHS,\n",
    "                    validation_data=validation_batches) \n",
    "        \n",
    "\n",
    "        \n",
    "plotter(history)\n",
    "        \n",
    "loss, accuracy = model.evaluate(testing_batches)\n",
    "        \n",
    "       \n",
    "print('\\nLoss on the TEST Set: {:,.3f}'.format(loss))\n",
    "print('\\nAccuracy on the TEST Set: {:.3%}'.format(accuracy))\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
